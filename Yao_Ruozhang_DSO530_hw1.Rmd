---
output:
  pdf_document: 
    latex_engine: xelatex
  html_document: default
---
# Homework 1
##Conceptual

## 1
Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV,
radio, and newspaper, rather than in terms of the coefficients of the linear model.

**Answer:** 
\newline Multiple regression coefficient estimates when TV, radio, and newspaper advertising budgets are used to predict product sales:
$$sales = \beta_0 + \beta_1 TV + \beta_2 radio + \beta_3 newspaper+ \epsilon $$
three null hypothesis: 
\newline $H_0: \beta_1= 0$, in the presence of radio and newspaper ads, TV ads have no relationship with sales.
\newline $H_0: \beta_2= 0$, in the presence of TV and newspaper ads, radio ads have no relationship with sales.
\newline $H_0: \beta_3= 0$, in the presence of TV and radio ads, newspaper ads have no relationship with sales.

The low p-values(<0.0001) of TV and radio show that the null hypotheses $H_0: \beta_1= 0$ and $H_0: \beta_2= 0$ are rejected for TV and radio, meaning TV and radio advertising budgets may make impacts on sales. While p-value of newspaper is high, suggesting that we do not reject null hypothesis $H_0: \beta_3= 0$ for newspaper. We may conclude that newspaper advertising budget do not affect sales.



## 3
Suppose we have a data set with five predictors, $X_1 =GPA$, $X_2 = IQ$, $X_3 = Gender$ (1 for Female and 0 forMale), $X_4$ = Interaction between GPA and IQ, and $X_5$ = Interaction between GPA and Gender. The
response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get $\hat{\beta}_0 = 50$,  $\hat{\beta}_1 = 20$,  $\hat{\beta}_2 = 0.07$,  $\hat{\beta}_3 = 35$,  $\hat{\beta}_4 = 0.01$,  $\hat{\beta}_5 = -10$.

(a) Which answer is correct, and why?
 i. For a fixed value of IQ and GPA, males earn more on average than females.
 ii. For a fixed value of IQ and GPA, females earn more on average than males.
 iii. For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.
 iv. For a fixed value of IQ and GPA, females earn more onaverage than males provided that the GPA is high enough.

**Answer:** 
\newline iii.For a fixed value of IQ and GPA, males earn more on average than females provided that the GPA is high enough.
\newline Regression equation:
$$\hat y_0 = 50 + 20 GPA+ 0.07 IQ+ 35 Gender+ 0.01 GPA * IQ - 10GPA*Gender$$ 

When Gender = 0 (Male):
$$\hat y_0 = 50 + 20 GPA + 0.07 IQ + 0.01 GPA * IQ$$ 
When Gender = 1 (Female):
$$\hat y_0 = 85 + 10 GPA + 0.07 IQ + 0.01 GPA * IQ$$ 
if:
$$ 50+20GPA≥85+10GPA$$
which is equivalent to $GPA≥3.5$, males earn more on avearge than females. Therefore (iii.) is the right answer.

(b) Predict the salary of a female with IQ of 110 and a GPA of 4.0.

**Answer:** 
\newline because $IQ = 110$, $Gender = 1$, and $GPA=4$, $ŷ =85+40+7.7+4.4=137.1$, $Predicted Salary = 137.1$

(c) True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.

**Answer:** 
\newline False. To check if the GPA/IQ has a relationship with salary we need to test the hypothesis $H_0:\hat{\beta_4}=0$ and look at the p-value to draw a conclusion. Because the p-value of the GPA/IQ interaction term is unknown, we cannot say the evidence of an interaction effect is little.


## 4
I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \beta_0 + \beta_1 X + \beta_2 X^2+ \beta_3 X^3+ \epsilon$

(a) Suppose that the true relationship between X and Y is linear, i.e. $Y = \beta_0 + \beta_1 X + \epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training
RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

 **Answer:** 
\newline For the training data, it is difficult to tell. Cubic regression is a more flexible model, so the RSS in the training data is expected to be smaller compared with a linear regression. the RSS will decrease when adding more explanatory variables for training data, so the RSS for the cubic model will be lower than the RSS for the linear model.

(b) Answer (a) using test rather than training RSS.

 **Answer:** 
\newline For test data, Linear regression correctly assumes the true data generating process, and the cubic model will be more overfitted than the linear model, so the RSS for the linear model will be lower than the RSS for the cubic model.

## 5
Consider the fitted values that result from performing linear regression without an intercept. In this setting, the ith fitted value takes the form:
$$\hat{y_i} = x_i \hat{\beta_i},$$
where
$$\hat{\beta} =({\sum^n_{i=1}x_iy_i}) / ({\sum^n_{i'=1}x^2_{i'}})  \quad \quad(3.38)$$   
Show that we can write
$$\hat{y_i} = {\sum^n_{i'=1}a_i'y_i'} $$
What is $a_i'$?
*Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.*

 **Answer:** 
$$\hat{y_i} = x_i\hat {\beta_i}$$
$$ y = x_i ({\sum^n_{i=1}x_i y_i}) / ({\sum^n_{i'=1}x^2_{i'}}) = x_i({\sum^n_{i'=1}x_i'y_i'})/({\sum^n_{k=1}x^2_{k}}) = \frac{\sum^n_{i'=1}x_ix_i'y_i'}{{\sum^n_{k=1}x^2_{k}}}=
{\sum^n_{i'=1}\frac{x_ix_i'y_i'}{{\sum^n_{k=1}x^2_{k}}}} = {\sum^n_{i'=1}\frac{x_ix_i'}{{\sum^n_{k=1}x^2_{k}}}}y_i' $$

so $$\hat{y_i} = {\sum^n_{i'=1}a_i'y_i'} $$
so $$a_i' = \frac{x_ix_i'}{{\sum^n_{k=1}x^2_{k}}} $$

## 6 
Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point $(\bar{x}, \bar{y})$

**Answer:** 
\newline From (3.4), we can know that least square line equation is $y = \hat{\beta_0}+\hat{\beta_1}x$
\newline and $\beta_0 = \bar{y} - \hat{\beta_1}\bar{x}$
\newline so $\hat{y} =\beta_0 + \beta_1 \bar{x}  = \bar{y} - \beta_1 \bar{x} + - \beta_1 \bar{x} = \bar{y}$
\newline the least squares line always passes through the point $(\bar{x}, \bar{y})$


##7
It is claimed in the text that in the case of simple linear regression of Y onto X, the $R^2$ statistic (3.17) is equal to the square of the correlation between X and Y (3.18). Prove that this is the case. For simplicity, you may assume that $\bar{x} = \bar{y} = 0$

**Answer:** 
\newline from (3.17), we know that:
because $\bar{x} = \bar{y} = 0$

\begin{equation}
\begin{split}
R^2 &= \frac{TSS-RSS}{TSS} = 1 - \frac{RSS}{TSS}  = 1- \frac{{\sum^n_{i=1}(y_i-\hat {y_i})^2}}{{\sum^n_{i=1}(y_i-\bar {y})^2}}\\
&= 1 - \frac{{\sum^n_{i=1}(y_i-\bar{y} + \hat{\beta_1}\bar{x}-\frac{{\sum^n_{i=1}(x_i-\bar {x_i})(y_i-\bar {y_i})}}{{\sum^n_{i=1}(x_i-\bar {x_i})^2}}x_i^2}}  {{\sum^n_{i=1}(y_i-\bar {y})^2}} \\
&= 1- \frac{{\sum^n_{i=1}(y_i - \frac{{\sum^n_{i=1}(x_i)(y_i)}}{{\sum^n_{i=1}(x_i)^2}}x_i )^2}}{{\sum^n_{i=1}(y_i)^2}}\\
\end{split}
\end{equation}

\begin{equation}
\begin{split}
R^2&= \frac{{\sum^n_{i=1}(y_i)^2} -{\sum^n_{i=1}(y_i - \frac{{\sum^n_{i=1}(x_i)(y_i)}}{{\sum^n_{i=1}x_i^2}}x_i )^2}}{{\sum^n_{i=1}(y_i)^2}}\\
&= \frac{2{\sum^n_{i=1}x_iy_i}\frac{{\sum^n_{i=1}(x_i)(y_i)}}{{\sum^n_{i=1}x_i^2}} - \frac{{(\sum^n_{i=1}x_iy_i)^2}}{{\sum^n_{i=1}x_i^2}}}{{\sum^n_{i=1}(y_i)^2}}\\
&= \frac{2*\frac{{(\sum^n_{i=1}x_iy_i)^2}}{{\sum^n_{i=1}x_i^2}} - \frac{{(\sum^n_{i=1}x_iy_i)^2}}{{\sum^n_{i=1}x_i^2}}}{{\sum^n_{i=1}(y_i)^2}}\\
&=\frac{{(\sum^n_{i=1}x_iy_i)^2}/{{\sum^n_{i=1}x_i^2}}}{{\sum^n_{i=1}(y_i)^2}}\\
\end{split}
\end{equation}
so
$$R^2= \frac{{(\sum^n_{i=1}x_iy_i)^2}}{{\sum^n_{i=1}x_i^2\sum^n_{i=1}y_i^2}} = Cor(X,Y)^2$$
Proven.

##Applied
##8
This question involves the use of simple linear regression on the Auto data set.
\newline (a) Use the **lm()** function to perform a simple linear regression with **mpg** as the response and **horsepower** as the predictor. Use the **summary()** function to print the results. Comment on the output. For example:
\newline i. Is there a relationship between the predictor and the response?
\newline ii. How strong is the relationship between the predictor and the response?
\newline iii. Is the relationship between the predictor and the response? positive or negative?
\newline iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?

**Answer:** 
```{r, message=FALSE}
library(ISLR)
data(Auto)
summary(Auto)

```

```{r, message=FALSE}
library(ISLR)
lm.fit = lm(mpg ~ horsepower, data = Auto)
summary(lm.fit)
```
i. Yes, there is a relationship between horsepower and mpg as deterined by testing the null hypothesis of all regression coefficients equal to zero. Since the p-value of the F-statistic is smaller than 0.05, we can reject the null hypothesis and conclude that there is a relationship.

ii. The adjusted R-squared of this model is 0.6049, which means 60.49% of the variance in the **mpg** can be explained by the variance in the predictor **horsepower**.

iii. The coeficient of “horsepower” is negative, so the relationship between the predictor and the response is negative.

iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?
```{r}
predict(lm.fit, data.frame(horsepower = 98), interval = 'confidence')
```
The predicted mpg is 24.47 associated with a horsepower of 98. Associated 95% confidence interval is [23.97, 24.96].
```{r}
predict(lm.fit, data.frame(horsepower = 98), interval = 'prediction')
```
Associated 95% prediction interval is [14.81, 34.12].

 (b) Plot the response and the predictor. Use the abline() function to display the least squares regression line.
```{r, message=FALSE}
plot(Auto$horsepower, Auto$mpg, main = "Scatterplot of mpg vs. horsepower", 
     xlab = "horsepower", ylab = "mpg") +
abline(lm.fit, col = 'red')
```

(c) Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.
```{r}
par(mfrow=c(2,2))
plot(lm.fit)
```
From the plot of residuals versus fitted values, we can find that there is some evidence of non-linearity. The plot of standardized residuals versus leverage shows there are some outliers.

## 9
This question involves the use of multiple linear regression on the Auto data set.
\newline (a) Produce a scatterplot matrix which includes all of the variables in the data set.
```{r}
pairs(Auto)
```

(b) Compute the matrix of correlations between the variables using the function **cor()**. You will need to exclude the name variable, cor() which is qualitative.
```{r}
Auto_new = Auto[1:8]
cor(Auto_new)
```
(c) Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as
the predictors. Use the summary() function to print the results. Comment on the output. For instance:
 i. Is there a relationship between the predictors and the response?
 ii. Which predictors appear to have a statistically significant relationship to the response?
 iii. What does the coefficient for the year variable suggest?

**Answer:**
```{r}
lm.fit9 = lm(formula = mpg ~ ., data = Auto_new)
summary(lm.fit9)
```
 i. Yes, there is a relationship between predictors and mpg. Since the p-value of the F-statistic is smaller than 0.05, we can reject the null hypothesis and conclude that there is a relationship.
 ii. Displacement, weight, year, and origin have a statistically significant relationship, while cylinders, horsepower, and acceleration do not.
 iii. Given all other predictors remaining constant, the average effect of an increase of 1 year is an increase of 0.7507727 in “mpg”.
 
(d) Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?
```{r}
par(mfrow = c(2, 2))
plot(lm.fit9)
```
As in question 8, the plot of residuals versus fitted values indicates non linearity in the data. The plot of standardized residuals versus leverage shows there are some outliers and one leverage point (14).

(e) Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?
```{r}
lm.fit9e = lm(mpg~cylinders*displacement+displacement*weight, data= Auto_new)
summary(lm.fit9e)
```
From the p-values, we can see that the interaction between displacement and weight is statistically signifcant.

(f) Try a few different transformations of the variables, such as $log(X)$,$\sqrt X$, $X^2$. Comment on your findings.
```{r}
lm.fit9f = lm(mpg ~ displacement+log(horsepower)+ log(weight)+sqrt(acceleration)+year+origin, data = Auto_new)
summary(lm.fit9f)
```
I try some different transformations of the variables that *log(horsepower),log(weight), sqrt(acceleration)*.The result shows Adjusted R-squared increases and only *sqrt(acceleration^2)* variable is not statistically significant.

## 10
This question should be answered using the Carseats data set.
\newline (a) Fit a multiple regression model to predict Sales using Price, Urban, and US.
```{r}
data(Carseats)
lm.fit10 <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(lm.fit10)
```

```{r}
summary(Carseats)
```
 (b) Provide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!
\newline Based on the coefficients in the model:
\newline **Price**: Sales decreases/increases 0.054 when Price increases/decreases 1, given other variables constant.
\newline **Urban**: If the store is in urban area versue not urban area, it would decrease Sales by 0.022, given other variables constant.
\newline **US**: If the store is in US, it would increase Sales by 1.200, given other variables constant.

 (c) Write out the model in equation form, being careful to handle the qualitative variables properly.
 $$Sales = 13.043 -0.054 Price -0.022 Urban + 1.200 US$$
 with $Urban=1$ if the store is in an urban area and 0 if not, and $US=1$ if the store is in the US and 0 if not.
 
 (d) For which of the predictors can you reject the null hypothesis $H_0 :\beta_j = 0$?
\newline Based on the p-value for each predictors, we can reject the null hypothesis of Price and US.

 (e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.
```{r}
lm.fit10e = lm(Sales ~ Price + US, data = Carseats)
summary(lm.fit10e)
```
 (f) How well do the models in (a) and (e) fit the data?
 \newline Based on the RSE and $R^2$ of the two linear regressions, they both fit the data similarly, while $R^2$ for the linear regression from (e) (23.54%) is marginally better than for the linear regression from (d).
 
 (g) Using the model from (e), obtain 95% confidence intervals for the coefficient(s).
```{r}
confint(lm.fit10e)
```

 (h) Is there evidence of outliers or high leverage observations in the model from (e)?
```{r}
par(mfrow=c(2,2))
plot(lm.fit10e)
```
There are some outliers (standardized residuals > 2 or < -2)) and some leverage points as some points exceed (p+1)/n(0.0076).
 